{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TOOL WEAR PREDICTION\n\n> **Note:** This notebook contains explanatory text in both English and Basque (Euskara). Each explanation appears first in English, followed by the original Basque version.\n\n---\n\n**[EN]** Hello, we are Gorka Dab\u00f3 and Mikel Oscoz. This notebook contains the implementation of the Tool Wear Prediction project with detailed explanations. Let's start by importing the required packages.\n\n**[EU]** Kaixo, Gorka Dab\u00f3 eta Mikel Oscoz gara eta koaderno honetan Tool Wear Prediction proiektuaren inplementazioa azalpenekin batera dago. Hasteko pakete batzuk inportatuko ditugu."
      ],
      "metadata": {
        "id": "RKX8BmQiMTPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v54ycgb85cXk",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770076404,
          "user_tz": -60,
          "elapsed": 20846,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orain kaggle-ko dataseta deskargatuko dugu, atzigarria izateko drive-ko karpeta publiko batean jarri ditugu datuak."
      ],
      "metadata": {
        "id": "x9J15xp7MqRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRCLTxuE5eF2",
        "outputId": "1158b8fb-6eb8-42bf-89bf-08ce3bffad1b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770168032,
          "user_tz": -60,
          "elapsed": 78855,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Dataset-IANS/Kaggle Dataset/experiment_01.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_02.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_03.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_04.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_05.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_06.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_07.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_08.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_09.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_10.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_11.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_12.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_13.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_14.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_15.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_16.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_17.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/experiment_18.csv',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/README.txt',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/test_artifact.jpg',\n",
              " '/content/Dataset-IANS/Kaggle Dataset/train.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import gdown\n",
        "url = \"https://drive.google.com/drive/folders/1GMP4ocr_x4ULEgRogHooFvrl81WoQju-?usp=sharing\"\n",
        "gdown.download_folder(url, quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[EN]** Now we will download the Kaggle dataset. To make it accessible, we have placed the data in a public Google Drive folder.\n\n**[EU]** Orain kaggle-ko dataseta deskargatuko dugu, atzigarria izateko drive-ko karpeta publiko batean jarri ditugu datuak."
      ],
      "metadata": {
        "id": "oY8GzDy4NH6B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ddbf_-Bx6N5m",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770168033,
          "user_tz": -60,
          "elapsed": 11,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    results = pd.read_csv(\"/content/Dataset-IANS/Kaggle Dataset/train.csv\")\n",
        "    worn_experiments = []\n",
        "    unworn_experiments = []\n",
        "\n",
        "    experiment_condition = {}\n",
        "\n",
        "    for i in range(1, 19):\n",
        "        exp = '0' + str(i) if i < 10 else str(i)\n",
        "        frame = pd.read_csv(f\"/content/Dataset-IANS/Kaggle Dataset/experiment_{exp}.csv\")\n",
        "        row = results[results['No'] == i]\n",
        "        condition = row.iloc[0]['tool_condition']\n",
        "\n",
        "\n",
        "        experiment_condition[i] = 1 if condition == 'worn' else 0\n",
        "\n",
        "        if condition == 'worn':\n",
        "            worn_experiments.append((i, frame))\n",
        "        else:\n",
        "            unworn_experiments.append((i, frame))\n",
        "\n",
        "    return worn_experiments, unworn_experiments, experiment_condition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funtzio honek, datu guztiak 3 multzotan banatzen ditu: train, dev eta test multzoak. Multzo bakoitzean bi klasetako esperimentuak egoteko eginda dago. Azkenik hiru listak bueltatzen ditu."
      ],
      "metadata": {
        "id": "0zsfJLqfNr95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w-mjFeFf6rrd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770168033,
          "user_tz": -60,
          "elapsed": 9,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [],
      "source": [
        "def split_experiments():\n",
        "    worn_experiments, unworn_experiments, experiment_condition = load_data()\n",
        "\n",
        "    test_worn = worn_experiments[:2]\n",
        "    test_unworn = unworn_experiments[:2]\n",
        "\n",
        "    remaining_worn = worn_experiments[2:]\n",
        "    remaining_unworn = unworn_experiments[2:]\n",
        "\n",
        "    val_worn = remaining_worn[:2]\n",
        "    val_unworn = remaining_unworn[:1]\n",
        "\n",
        "    train_worn = remaining_worn[2:]\n",
        "    train_unworn = remaining_unworn[1:]\n",
        "\n",
        "    return (train_worn + train_unworn,\n",
        "            val_worn + val_unworn,\n",
        "            test_worn + test_unworn,\n",
        "            experiment_condition)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[EN]** The `load_data` function loads the necessary data for tool wear analysis. First, it reads a CSV file containing information about each experiment's tool condition (worn or unworn).\n\nThen, it loads the data for each experiment and finally returns three elements: the worn experiments, the unworn experiments, and a dictionary mapping experiment IDs to their conditions.\n\n**[EU]** Load_data funtzioak erreminten higadura aztertzeko beharrezko datuak kargatzen ditu. Lehenik eta behin, irakurri CSV fitxategi bat, esperimentu bakoitzerako tresnaren egoerari buruzko informazioa duena (higatua edo ez).\n\nOndoren, esperimentu bakoitzaren datuak kargatzen ditu eta azkenik, funtzioak hiru elementu itzultzen ditu: higatutako esperimentuak, higatu gabekoak eta esperimenturen kondizioak mapeatzen dituen hiztegia."
      ],
      "metadata": {
        "id": "v99RtWJLOelx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XuPYwPiX6wSL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770168033,
          "user_tz": -60,
          "elapsed": 9,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [],
      "source": [
        "def prepare_data(experiments):\n",
        "    all_data = []\n",
        "    experiment_indices = []\n",
        "\n",
        "    for exp_id, frame in experiments:\n",
        "        frame = frame.drop('Machining_Process', axis=1)\n",
        "        n_rows = len(frame)\n",
        "        all_data.append(frame)\n",
        "        experiment_indices.extend([exp_id] * n_rows)\n",
        "\n",
        "    return pd.concat(all_data, ignore_index=True), experiment_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`create_sliding_windows_by_experiment` Funtzio honek leiho irristakorrak sortzen ditu entrenamendu eta balidazio multzoetarako. Esperimentu bakoitzaren datuak tamaina finkoko leihoetan banatzen ditu, eta urrats jakin batekin mugitzen da. Leiho horiek modeloa entrenatzeko erabiliko dira, datuetan aldi baterako patroiak atzemateko aukera emanez. Funtzioak array bat itzultzen du sortutako leihoekin eta beste bat leihoak dagozkien esperimentuen IDekin."
      ],
      "metadata": {
        "id": "B3Yy8urWO4H5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AuszW0nQ62Hj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770168033,
          "user_tz": -60,
          "elapsed": 8,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [],
      "source": [
        "def create_sliding_windows_by_experiment(data, experiment_indices, window_size, step_size):\n",
        "    windows = []\n",
        "    window_exp_ids = []\n",
        "\n",
        "    unique_experiments = np.unique(experiment_indices)\n",
        "\n",
        "    for exp_id in unique_experiments:\n",
        "        exp_mask = np.array(experiment_indices) == exp_id\n",
        "        exp_data = data[exp_mask]\n",
        "\n",
        "        if len(exp_data) >= window_size:\n",
        "            valid_indices = range(len(exp_data) - window_size + 1)\n",
        "            for i in range(0, len(valid_indices), step_size):\n",
        "                if i + window_size <= len(exp_data):\n",
        "                    windows.append(exp_data[i:i + window_size])\n",
        "                    window_exp_ids.append(exp_id)\n",
        "\n",
        "    return np.array(windows), window_exp_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[EN]** This function splits all data into 3 sets: train, dev, and test sets. It is designed to ensure that experiments from both classes are present in each set. Finally, it returns three lists.\n\n**[EU]** Funtzio honek, datu guztiak 3 multzotan banatzen ditu: train, dev eta test multzoak. Multzo bakoitzean bi klasetako esperimentuak egoteko eginda dago. Azkenik hiru listak bueltatzen ditu."
      ],
      "metadata": {
        "id": "_S194I8aPHxA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IrGZuwok63tr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770168033,
          "user_tz": -60,
          "elapsed": 8,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [],
      "source": [
        "def create_test_windows_by_experiment(data, experiment_indices, window_size):\n",
        "    windows = []\n",
        "    window_exp_ids = []\n",
        "\n",
        "    unique_experiments = np.unique(experiment_indices)\n",
        "\n",
        "    for exp_id in unique_experiments:\n",
        "        exp_mask = np.array(experiment_indices) == exp_id\n",
        "        exp_data = data[exp_mask]\n",
        "\n",
        "        current_pos = 0\n",
        "        while current_pos + window_size <= len(exp_data):\n",
        "            windows.append(exp_data[current_pos:current_pos + window_size])\n",
        "            window_exp_ids.append(exp_id)\n",
        "            current_pos += window_size\n",
        "\n",
        "    return np.array(windows), window_exp_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`HybridCNNRNN` klaseak Sare Neuronal Konboluzionalen (CNN) eta Sare Neuronal Errekurrenteen (RNN) geruzak konbinatzen dituen eredu hibrido baten arkitektura definitzen du, zehazki LSTM (Long Short-Term Memory). Eredu mota hau oso erabilgarria da da ezaugarri espazialak dituzten datu sekuentzialak prozesatzeko, hala nola tresnen higadura-datuak.\n",
        "\n",
        "Ereduak osagai hauek ditu:\n",
        "\n",
        "* **CNN geruzak:** sarrerako datuetatik ezaugarri lokalak eta patroi\n",
        "espazialak ateratzeko erabiltzen dira.\n",
        "* **LSTM geruzak:** Denbora-mendekotasunak eta epe luzerako patroiak atzematen dituzte datu-sekuentzietan.\n",
        "* **Fully Connected geruzak:** Azken sailkapenaz arduratzen dira, ereduaren iragarpena irudikatzen duen irteera baten aurreko geruzek ateratako ezaugarriak mapeatuz.\n"
      ],
      "metadata": {
        "id": "VyBM1BO1PcaR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "piq9thK865My",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770168034,
          "user_tz": -60,
          "elapsed": 8,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [],
      "source": [
        "class HybridCNNRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # CNN geruzak\n",
        "        self.conv1 = nn.Conv1d(in_channels=47, out_channels=32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        # LSTM geruzak\n",
        "        self.rnn = nn.LSTM(input_size=64, hidden_size=128,\n",
        "                          num_layers=2, batch_first=True,\n",
        "                          bidirectional=True, dropout=0.3)\n",
        "\n",
        "        # Fully connected geruzak\n",
        "        self.fc1 = nn.Linear(256, 64)  # 256 bidirekzioanala delako (128*2)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # LSTM\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # Fully connected\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[EN]** The `prepare_data` function preprocesses the experiment data, combining them for later use.\n\n**[EU]** `prepare_data` funtzioak esperimentuen datuak preprozesatzen ditu, hauek konbinatzen gero erabili ahal izateko."
      ],
      "metadata": {
        "id": "6B6-49IyQEmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "svwqBse5_39z",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770168034,
          "user_tz": -60,
          "elapsed": 8,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [],
      "source": [
        "def train_loop(model, train_loader, val_loader, device, num_epochs=10, loocv=False):\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "\n",
        "    best_val_f1 = -1\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}:')\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_preds = []\n",
        "        train_targets = []\n",
        "\n",
        "        size = len(train_loader.dataset)\n",
        "\n",
        "        for batch, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            pred = model(X_batch)\n",
        "            loss = loss_fn(pred.squeeze(), y_batch.float())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_preds.extend((pred.squeeze() > 0.5).int().cpu().numpy())\n",
        "            train_targets.extend(y_batch.cpu().numpy())\n",
        "\n",
        "            if loocv == False and batch % 20 == 0:\n",
        "                loss, current = loss.item(), batch * len(X_batch)\n",
        "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "        # Balidazioa\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch = X_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                pred = model(X_batch)\n",
        "                val_preds.extend((pred.squeeze() > 0.5).int().cpu().numpy())\n",
        "                val_targets.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        # Metrikak kalkulatu\n",
        "        train_f1 = f1_score(train_targets, train_preds)\n",
        "        val_f1 = f1_score(val_targets, val_preds)\n",
        "\n",
        "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train F1: {train_f1:.4f}')\n",
        "        print(f'Val F1: {val_f1:.4f}')\n",
        "\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_model = model.state_dict().copy()\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Evaluate_model` funtzioak test multzoan entrenatutako ereduaren errendimendua ebaluatzen du. Helburua da ereduak entrenamenduan ikusi gabeko datuetan erreminten higadura aurreikusteko duen gaitasuna adierazten duten metrikak lortzea.\n",
        "\n",
        "Horretarako, lehenengo funtzioak test multzoari buruzko iragarpenak egiten ditu. Ondoren, leiho-mailan eta esperimentu-mailan metrikak kalkulatzen ditu. Metrika kalkulatuek zehaztasuna, recall eta F1-score dira. Azkenik, funtzioak kalkulatutako metrikak, leiho eta esperimentu mailako iragarpenak eta esperimentuen benetako etiketak biltzen dituen hiztegia itzultzen du."
      ],
      "metadata": {
        "id": "sAVK6kYiQ2zx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cOA2e980_4VX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770168034,
          "user_tz": -60,
          "elapsed": 7,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        }
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, test_experiment_indices, device):\n",
        "    model.eval()\n",
        "    predictions_by_experiment = {}\n",
        "    true_labels_by_experiment = {}\n",
        "\n",
        "    # Lehioen metrikak\n",
        "    all_window_predictions = []\n",
        "    all_window_true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_start = 0\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            pred = model(X_batch)\n",
        "            predictions = (pred.squeeze() > 0.5).int().cpu().numpy()\n",
        "\n",
        "            all_window_predictions.extend(predictions)\n",
        "            all_window_true_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "            for i, pred in enumerate(predictions):\n",
        "                exp_id = test_experiment_indices[batch_start + i]\n",
        "                if exp_id not in predictions_by_experiment:\n",
        "                    predictions_by_experiment[exp_id] = []\n",
        "                    true_labels_by_experiment[exp_id] = y_batch[i].item()\n",
        "                predictions_by_experiment[exp_id].append(pred)\n",
        "\n",
        "            batch_start += len(y_batch)\n",
        "\n",
        "    window_metrics = {\n",
        "        'accuracy': accuracy_score(all_window_true_labels, all_window_predictions),\n",
        "        'precision': precision_score(all_window_true_labels, all_window_predictions),\n",
        "        'recall': recall_score(all_window_true_labels, all_window_predictions),\n",
        "        'f1': f1_score(all_window_true_labels, all_window_predictions)\n",
        "    }\n",
        "\n",
        "    # Esperimentu bakoitzako majority voting egin\n",
        "    experiment_predictions = []\n",
        "    experiment_true_labels = []\n",
        "\n",
        "    for exp_id in predictions_by_experiment:\n",
        "        exp_predictions = predictions_by_experiment[exp_id]\n",
        "        # Majority voting\n",
        "        majority_vote = Counter(exp_predictions).most_common(1)[0][0]\n",
        "        experiment_predictions.append(majority_vote)\n",
        "        experiment_true_labels.append(true_labels_by_experiment[exp_id])\n",
        "\n",
        "    experiment_metrics = {\n",
        "        'accuracy': accuracy_score(experiment_true_labels, experiment_predictions),\n",
        "        'precision': precision_score(experiment_true_labels, experiment_predictions),\n",
        "        'recall': recall_score(experiment_true_labels, experiment_predictions),\n",
        "        'f1': f1_score(experiment_true_labels, experiment_predictions)\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'window_metrics': window_metrics,\n",
        "        'experiment_metrics': experiment_metrics,\n",
        "        'window_predictions': all_window_predictions,\n",
        "        'window_true_labels': all_window_true_labels,\n",
        "        'experiment_predictions': experiment_predictions,\n",
        "        'experiment_true_labels': experiment_true_labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[EN]** `create_sliding_windows_by_experiment` - This function creates sliding windows for training and validation sets. It divides each experiment's data into fixed-size windows, moving with a specific step size. These windows will be used to train the model, enabling the detection of temporal patterns in the data. The function returns an array with the generated windows and another with the experiment IDs corresponding to each window.\n\n**[EU]** `create_sliding_windows_by_experiment` Funtzio honek leiho irristakorrak sortzen ditu entrenamendu eta balidazio multzoetarako. Esperimentu bakoitzaren datuak tamaina finkoko leihoetan banatzen ditu, eta urrats jakin batekin mugitzen da. Leiho horiek modeloa entrenatzeko erabiliko dira, datuetan aldi baterako patroiak atzemateko aukera emanez. Funtzioak array bat itzultzen du sortutako leihoekin eta beste bat leihoak dagozkien esperimentuen IDekin."
      ],
      "metadata": {
        "id": "qQ_L6n6zTVmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_9iD64rQAESG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736770177979,
          "user_tz": -60,
          "elapsed": 9952,
          "user": {
            "displayName": "Gorka Dab\u00f3",
            "userId": "01902021814276509841"
          }
        },
        "outputId": "a0ee2257-f13d-426b-a204-5552a984bf2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "\n",
            "Epoch 1:\n",
            "loss: 0.683587  [    0/ 1649]\n",
            "loss: 0.675646  [  640/ 1649]\n",
            "loss: 0.554105  [ 1280/ 1649]\n",
            "Train Loss: 0.6585, Train F1: 0.7167\n",
            "Val F1: 0.6414\n",
            "\n",
            "Epoch 2:\n",
            "loss: 0.581713  [    0/ 1649]\n",
            "loss: 0.595820  [  640/ 1649]\n",
            "loss: 0.745361  [ 1280/ 1649]\n",
            "Train Loss: 0.5417, Train F1: 0.7389\n",
            "Val F1: 0.6111\n",
            "\n",
            "Epoch 3:\n",
            "loss: 0.408757  [    0/ 1649]\n",
            "loss: 0.374869  [  640/ 1649]\n",
            "loss: 0.627431  [ 1280/ 1649]\n",
            "Train Loss: 0.3997, Train F1: 0.8314\n",
            "Val F1: 0.6232\n",
            "\n",
            "Epoch 4:\n",
            "loss: 0.380280  [    0/ 1649]\n",
            "loss: 0.432512  [  640/ 1649]\n",
            "loss: 0.338392  [ 1280/ 1649]\n",
            "Train Loss: 0.2678, Train F1: 0.9084\n",
            "Val F1: 0.6193\n",
            "\n",
            "Epoch 5:\n",
            "loss: 0.249256  [    0/ 1649]\n",
            "loss: 0.282770  [  640/ 1649]\n",
            "loss: 0.780990  [ 1280/ 1649]\n",
            "Train Loss: 0.2553, Train F1: 0.9173\n",
            "Val F1: 0.6162\n",
            "\n",
            "Epoch 6:\n",
            "loss: 0.340773  [    0/ 1649]\n",
            "loss: 0.162852  [  640/ 1649]\n",
            "loss: 0.288304  [ 1280/ 1649]\n",
            "Train Loss: 0.1694, Train F1: 0.9439\n",
            "Val F1: 0.6111\n",
            "\n",
            "Epoch 7:\n",
            "loss: 0.046628  [    0/ 1649]\n",
            "loss: 0.034947  [  640/ 1649]\n",
            "loss: 0.029621  [ 1280/ 1649]\n",
            "Train Loss: 0.1054, Train F1: 0.9680\n",
            "Val F1: 0.6322\n",
            "\n",
            "Epoch 8:\n",
            "loss: 0.311327  [    0/ 1649]\n",
            "loss: 0.166072  [  640/ 1649]\n",
            "loss: 0.046447  [ 1280/ 1649]\n",
            "Train Loss: 0.0712, Train F1: 0.9775\n",
            "Val F1: 0.6215\n",
            "\n",
            "Epoch 9:\n",
            "loss: 0.020450  [    0/ 1649]\n",
            "loss: 0.051792  [  640/ 1649]\n",
            "loss: 0.024815  [ 1280/ 1649]\n",
            "Train Loss: 0.0671, Train F1: 0.9802\n",
            "Val F1: 0.6337\n",
            "\n",
            "Epoch 10:\n",
            "loss: 0.030508  [    0/ 1649]\n",
            "loss: 0.002445  [  640/ 1649]\n",
            "loss: 0.016884  [ 1280/ 1649]\n",
            "Train Loss: 0.0280, Train F1: 0.9936\n",
            "Val F1: 0.6286\n",
            "\n",
            "Testing model...\n",
            "Window-level Results:\n",
            "Accuracy: 0.4857\n",
            "Precision: 0.4333\n",
            "Recall: 0.9286\n",
            "F1-score: 0.5909\n",
            "\n",
            "Experiment-level Results (after majority voting):\n",
            "Accuracy: 0.7500\n",
            "Precision: 0.6667\n",
            "Recall: 1.0000\n",
            "F1-score: 0.8000\n",
            "\n",
            "Predictions for each experiment: [0, 1, 1, 1]\n",
            "True labels for experiments: [0.0, 0.0, 1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Hyperparametroak\n",
        "    window_size = 128\n",
        "    step_size = 10\n",
        "    batch_size = 32\n",
        "\n",
        "    train_experiments, val_experiments, test_experiments, experiment_condition = split_experiments()\n",
        "\n",
        "    # Datasetak prestatu\n",
        "    train_data, train_exp_indices = prepare_data(train_experiments)\n",
        "    val_data, val_exp_indices = prepare_data(val_experiments)\n",
        "    test_data, test_exp_indices = prepare_data(test_experiments)\n",
        "\n",
        "    # Datuak normalizatu\n",
        "    scaler = StandardScaler()\n",
        "    train_data_normalized = scaler.fit_transform(train_data)\n",
        "    val_data_normalized = scaler.transform(val_data)\n",
        "    test_data_normalized = scaler.transform(test_data)\n",
        "\n",
        "    # Lehioak sortu\n",
        "    X_train, train_window_exp_ids = create_sliding_windows_by_experiment(train_data_normalized, train_exp_indices, window_size, step_size)\n",
        "    X_val, val_window_exp_ids = create_sliding_windows_by_experiment(val_data_normalized, val_exp_indices, window_size, step_size)\n",
        "    X_test, test_window_exp_ids = create_test_windows_by_experiment(test_data_normalized, test_exp_indices, window_size)\n",
        "\n",
        "    y_train = np.array([experiment_condition[exp_id] for exp_id in train_window_exp_ids])\n",
        "    y_val = np.array([experiment_condition[exp_id] for exp_id in val_window_exp_ids])\n",
        "    y_test = np.array([experiment_condition[exp_id] for exp_id in test_window_exp_ids])\n",
        "\n",
        "    # Dataloaderrak sortu\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = HybridCNNRNN().to(device)\n",
        "\n",
        "    # Eredua entrenatu\n",
        "    print(\"Training model...\")\n",
        "    best_model_state = train_loop(model, train_loader, val_loader, device)\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    print(\"\\nTesting model...\")\n",
        "    results = evaluate_model(model, test_loader, test_window_exp_ids, device)\n",
        "\n",
        "    print(\"Window-level Results:\")\n",
        "    print(f\"Accuracy: {results['window_metrics']['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {results['window_metrics']['precision']:.4f}\")\n",
        "    print(f\"Recall: {results['window_metrics']['recall']:.4f}\")\n",
        "    print(f\"F1-score: {results['window_metrics']['f1']:.4f}\")\n",
        "\n",
        "    print(\"\\nExperiment-level Results (after majority voting):\")\n",
        "    print(f\"Accuracy: {results['experiment_metrics']['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {results['experiment_metrics']['precision']:.4f}\")\n",
        "    print(f\"Recall: {results['experiment_metrics']['recall']:.4f}\")\n",
        "    print(f\"F1-score: {results['experiment_metrics']['f1']:.4f}\")\n",
        "\n",
        "    print(\"\\nPredictions for each experiment:\", results['experiment_predictions'])\n",
        "    print(\"True labels for experiments:\", results['experiment_true_labels'])\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LEAVE-ONE-OUT CROSS VALIDATION"
      ],
      "metadata": {
        "id": "YxgYDB15McBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[EN]** `create_test_windows_by_experiment` - Similar to the previous one, this function creates data windows but is designed for the test set. The main difference is that it doesn't use a \"step\" for sliding the window, but instead creates consecutive windows without overlap. This ensures that all information from the experiment will be used in testing, avoiding biases in evaluation. Like the previous function, it returns an array with windows and another with their corresponding experiment IDs.\n\n**[EU]** `create_test_windows_by_experiment` Aurrekoaren antzekoa da, funtzio honek datu leihoak sortzen ditu, baina test multzorako diseinatuta dago. Alde nagusia da ez duela \"urrats\" bat erabiltzen leihoa irristatzeko, baizik eta elkarren segidako leihoak sortzen dituela gainjartzerik gabe. Horrek proban esperimentuaren informazio guztia erabiliko dela ziurtatzen du, ebaluazioan alborapenak saihestuz. Aurrekoak bezala, array bat itzultzen du leihoekin eta beste bat dagozkion esperimentuen IDekin."
      ],
      "metadata": {
        "id": "Bdi-IEC3URvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridCNNRNNNoWindow(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # CNN geruzak\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        # LSTM geruzak\n",
        "        self.rnn = nn.LSTM(input_size=64, hidden_size=128,\n",
        "                          num_layers=2, batch_first=True,\n",
        "                          bidirectional=True, dropout=0.3)\n",
        "\n",
        "        # Fully connected geruzak\n",
        "        self.fc1 = nn.Linear(256, 64)  # 256 bidirekzionala delako (128*2)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # CNN\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # LSTM\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # Fully connected\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qWbc9N-8Tgnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Loocv_evaluation` funtzioak \"Leave-One-Out\" (LOOCV) balidazio gurutzatuaren teknika inplementatzen du HybridCNNRNNoWindow eredua ebaluatzeko. Teknika hau eredua behin eta berriz entrenatzean eta ebaluatzean datza, esperimentu indibidual bakoitza test multzo gisa behin erabiliz eta gainerakoak entrenamendu multzo gisa erabiliz.\n",
        "\n",
        "Iterazio bakoitzean, funtzioak probarako esperimentu bat bereizten du, entrenamendu eta test datuak prestatzen ditu, datuak normalizatzen ditu eta leihorik gabeko datu multzoak sortzen ditu. Gero, eredua hasieratu eta entrenatzen du entrenamendu-datuak erabiliz, eta probako esperimentuan ebaluatzen du."
      ],
      "metadata": {
        "id": "4Jl20RTcUlsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loocv_evaluation():\n",
        "    # Esperimentu guztiak kargatu\n",
        "    worn_experiments, unworn_experiments, experiment_condition = load_data()\n",
        "\n",
        "    # Esperimentu guztiak konbinatu\n",
        "    all_experiments = worn_experiments + unworn_experiments\n",
        "\n",
        "    f1_scores = []\n",
        "    accuracies = []\n",
        "\n",
        "    # LOOCV\n",
        "    for test_exp_idx in range(len(all_experiments)):\n",
        "        print(f\"\\nTesting with experiment {all_experiments[test_exp_idx][0]}\")\n",
        "\n",
        "        # Banatu train eta test\n",
        "        test_exp = [all_experiments[test_exp_idx]]\n",
        "        train_exp = [exp for i, exp in enumerate(all_experiments) if i != test_exp_idx]\n",
        "\n",
        "        train_data, train_exp_indices = prepare_data(train_exp)\n",
        "        test_data, test_exp_indices = prepare_data(test_exp)\n",
        "\n",
        "        # Datuak normalizatu\n",
        "        scaler = StandardScaler()\n",
        "        train_data_normalized = scaler.fit_transform(train_data)\n",
        "        test_data_normalized = scaler.transform(test_data)\n",
        "\n",
        "        X_train = torch.FloatTensor(train_data_normalized)\n",
        "        y_train = torch.FloatTensor([experiment_condition[exp_id] for exp_id in train_exp_indices])\n",
        "\n",
        "        X_test = torch.FloatTensor(test_data_normalized)\n",
        "        y_test = torch.FloatTensor([experiment_condition[exp_id] for exp_id in test_exp_indices])\n",
        "\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = HybridCNNRNNNoWindow().to(device)\n",
        "\n",
        "        # Eredua entrenatu\n",
        "        try:\n",
        "            best_model_state = train_loop(model, train_loader, test_loader, device, num_epochs=10, loocv=True)\n",
        "            model.load_state_dict(best_model_state)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during training: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Ebaluazioa\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                X_batch = X_batch.to(device)\n",
        "                pred = model(X_batch)\n",
        "                predictions = (pred.squeeze() > 0.5).int().cpu().numpy()\n",
        "                all_preds.extend(predictions)\n",
        "                all_labels.extend(y_batch.numpy())\n",
        "\n",
        "        fold_f1 = f1_score(all_labels, all_preds)\n",
        "        fold_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        print(f\"Experiment {all_experiments[test_exp_idx][0]} - F1: {fold_f1:.4f}, Accuracy: {fold_accuracy:.4f}\")\n",
        "\n",
        "        f1_scores.append(fold_f1)\n",
        "        accuracies.append(fold_accuracy)\n",
        "\n",
        "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "    print(\"\\nOverall LOOCV Results:\")\n",
        "    print(f\"Average F1-Score: {avg_f1:.4f}\")\n",
        "    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\nIndividual Experiment Scores:\")\n",
        "    for i, (f1, acc) in enumerate(zip(f1_scores, accuracies)):\n",
        "        print(f\"Experiment {all_experiments[i][0]}: F1 = {f1:.4f}, Accuracy = {acc:.4f}\")\n",
        "\n",
        "    return f1_scores, accuracies\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Leave-One-Out Cross-Validation with Hybrid CNN-RNN...\")\n",
        "    f1_scores, accuracies = loocv_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF7ll2aRQaDO",
        "outputId": "d267f275-75ac-45d4-b464-710467d48bb2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1736767078549,
          "user_tz": -60,
          "elapsed": 774479,
          "user": {
            "displayName": "Mikel Oscoz",
            "userId": "06644477822200861186"
          }
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Leave-One-Out Cross-Validation with Hybrid CNN-RNN...\n",
            "\n",
            "Testing with experiment 6\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6362, Train F1: 0.5927\n",
            "Val F1: 0.7490\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5923, Train F1: 0.6355\n",
            "Val F1: 0.5672\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5580, Train F1: 0.6933\n",
            "Val F1: 0.5314\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5295, Train F1: 0.6972\n",
            "Val F1: 0.6019\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5055, Train F1: 0.7171\n",
            "Val F1: 0.4357\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.4853, Train F1: 0.7290\n",
            "Val F1: 0.5875\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4696, Train F1: 0.7394\n",
            "Val F1: 0.5078\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4567, Train F1: 0.7492\n",
            "Val F1: 0.3026\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4473, Train F1: 0.7552\n",
            "Val F1: 0.3114\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4310, Train F1: 0.7675\n",
            "Val F1: 0.3147\n",
            "Experiment 6 - F1: 0.3147, Accuracy: 0.1867\n",
            "\n",
            "Testing with experiment 7\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6377, Train F1: 0.6109\n",
            "Val F1: 0.8597\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5969, Train F1: 0.5729\n",
            "Val F1: 0.9289\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5732, Train F1: 0.6052\n",
            "Val F1: 0.8677\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5529, Train F1: 0.6286\n",
            "Val F1: 0.9299\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5325, Train F1: 0.6657\n",
            "Val F1: 0.8834\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.5187, Train F1: 0.6950\n",
            "Val F1: 0.8241\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4913, Train F1: 0.7393\n",
            "Val F1: 0.9145\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4715, Train F1: 0.7512\n",
            "Val F1: 0.7744\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4536, Train F1: 0.7652\n",
            "Val F1: 0.8516\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4408, Train F1: 0.7746\n",
            "Val F1: 0.7771\n",
            "Experiment 7 - F1: 0.7771, Accuracy: 0.6354\n",
            "\n",
            "Testing with experiment 8\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6401, Train F1: 0.6574\n",
            "Val F1: 1.0000\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.6016, Train F1: 0.6179\n",
            "Val F1: 0.9780\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5749, Train F1: 0.6539\n",
            "Val F1: 0.7678\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5508, Train F1: 0.6883\n",
            "Val F1: 0.7960\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5268, Train F1: 0.7132\n",
            "Val F1: 0.9685\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.5020, Train F1: 0.7392\n",
            "Val F1: 0.9216\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4820, Train F1: 0.7545\n",
            "Val F1: 0.8879\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4661, Train F1: 0.7594\n",
            "Val F1: 0.8960\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4499, Train F1: 0.7723\n",
            "Val F1: 0.8509\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4316, Train F1: 0.7832\n",
            "Val F1: 0.7888\n",
            "Experiment 8 - F1: 0.7888, Accuracy: 0.6512\n",
            "\n",
            "Testing with experiment 9\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6382, Train F1: 0.6179\n",
            "Val F1: 0.8374\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5949, Train F1: 0.6136\n",
            "Val F1: 0.8365\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5728, Train F1: 0.6599\n",
            "Val F1: 0.8383\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5542, Train F1: 0.6800\n",
            "Val F1: 0.8300\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5384, Train F1: 0.6945\n",
            "Val F1: 0.8554\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.5164, Train F1: 0.7071\n",
            "Val F1: 0.8365\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.5014, Train F1: 0.7259\n",
            "Val F1: 0.8356\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4880, Train F1: 0.7374\n",
            "Val F1: 0.8474\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4648, Train F1: 0.7614\n",
            "Val F1: 0.8642\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4559, Train F1: 0.7641\n",
            "Val F1: 0.8179\n",
            "Experiment 9 - F1: 0.8179, Accuracy: 0.6919\n",
            "\n",
            "Testing with experiment 10\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6427, Train F1: 0.5870\n",
            "Val F1: 0.9981\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.6165, Train F1: 0.5176\n",
            "Val F1: 0.9860\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.6047, Train F1: 0.5181\n",
            "Val F1: 0.9879\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5911, Train F1: 0.5798\n",
            "Val F1: 0.9788\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5609, Train F1: 0.6372\n",
            "Val F1: 0.9049\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.5313, Train F1: 0.6815\n",
            "Val F1: 0.8899\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.5127, Train F1: 0.7050\n",
            "Val F1: 0.8537\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4889, Train F1: 0.7313\n",
            "Val F1: 0.9384\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4637, Train F1: 0.7533\n",
            "Val F1: 0.9942\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4468, Train F1: 0.7670\n",
            "Val F1: 0.9808\n",
            "Experiment 10 - F1: 0.9808, Accuracy: 0.9623\n",
            "\n",
            "Testing with experiment 13\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6216, Train F1: 0.5666\n",
            "Val F1: 0.1794\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5611, Train F1: 0.6072\n",
            "Val F1: 0.0309\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5156, Train F1: 0.6493\n",
            "Val F1: 0.1644\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.4696, Train F1: 0.7305\n",
            "Val F1: 0.2710\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.4389, Train F1: 0.7621\n",
            "Val F1: 0.2071\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.4075, Train F1: 0.7870\n",
            "Val F1: 0.1992\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.3778, Train F1: 0.8096\n",
            "Val F1: 0.3223\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.3469, Train F1: 0.8282\n",
            "Val F1: 0.1361\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.3201, Train F1: 0.8452\n",
            "Val F1: 0.1282\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.3025, Train F1: 0.8575\n",
            "Val F1: 0.0939\n",
            "Experiment 13 - F1: 0.0939, Accuracy: 0.0493\n",
            "\n",
            "Testing with experiment 14\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6181, Train F1: 0.5695\n",
            "Val F1: 0.1292\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5636, Train F1: 0.5806\n",
            "Val F1: 0.1156\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5368, Train F1: 0.6137\n",
            "Val F1: 0.1845\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5143, Train F1: 0.6433\n",
            "Val F1: 0.2226\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.4925, Train F1: 0.6600\n",
            "Val F1: 0.0695\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.4783, Train F1: 0.6737\n",
            "Val F1: 0.3956\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4632, Train F1: 0.7012\n",
            "Val F1: 0.1717\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4494, Train F1: 0.7157\n",
            "Val F1: 0.3034\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4298, Train F1: 0.7327\n",
            "Val F1: 0.3384\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4183, Train F1: 0.7467\n",
            "Val F1: 0.4698\n",
            "Experiment 14 - F1: 0.4698, Accuracy: 0.3070\n",
            "\n",
            "Testing with experiment 15\n",
            "\n",
            "Epoch 1:\n",
            "Error during training: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n",
            "\n",
            "Testing with experiment 16\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6341, Train F1: 0.6069\n",
            "Val F1: 0.9512\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5854, Train F1: 0.5414\n",
            "Val F1: 0.1333\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5638, Train F1: 0.5451\n",
            "Val F1: 0.2898\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5454, Train F1: 0.6025\n",
            "Val F1: 0.0549\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5324, Train F1: 0.6640\n",
            "Val F1: 0.1304\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.5191, Train F1: 0.6862\n",
            "Val F1: 0.1840\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.5065, Train F1: 0.7159\n",
            "Val F1: 0.0859\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4915, Train F1: 0.7331\n",
            "Val F1: 0.4544\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4762, Train F1: 0.7476\n",
            "Val F1: 0.8138\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4642, Train F1: 0.7571\n",
            "Val F1: 0.7510\n",
            "Experiment 16 - F1: 0.7510, Accuracy: 0.6013\n",
            "\n",
            "Testing with experiment 18\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6192, Train F1: 0.5681\n",
            "Val F1: 0.1318\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5663, Train F1: 0.5898\n",
            "Val F1: 0.1178\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5326, Train F1: 0.6164\n",
            "Val F1: 0.1593\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5107, Train F1: 0.6529\n",
            "Val F1: 0.1608\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.4921, Train F1: 0.6628\n",
            "Val F1: 0.1683\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.4748, Train F1: 0.6754\n",
            "Val F1: 0.1162\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4597, Train F1: 0.6873\n",
            "Val F1: 0.2350\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4466, Train F1: 0.7142\n",
            "Val F1: 0.3348\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4323, Train F1: 0.7348\n",
            "Val F1: 0.4579\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4214, Train F1: 0.7453\n",
            "Val F1: 0.5065\n",
            "Experiment 18 - F1: 0.5065, Accuracy: 0.3391\n",
            "\n",
            "Testing with experiment 1\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6197, Train F1: 0.7031\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5771, Train F1: 0.6722\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5503, Train F1: 0.6939\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5307, Train F1: 0.7083\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5159, Train F1: 0.7223\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.5014, Train F1: 0.7298\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4861, Train F1: 0.7489\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4757, Train F1: 0.7608\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4584, Train F1: 0.7744\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4485, Train F1: 0.7804\n",
            "Val F1: 0.0000\n",
            "Experiment 1 - F1: 0.0000, Accuracy: 0.5867\n",
            "\n",
            "Testing with experiment 2\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6416, Train F1: 0.6351\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5969, Train F1: 0.6485\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5803, Train F1: 0.6856\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5580, Train F1: 0.7077\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5362, Train F1: 0.7297\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.5149, Train F1: 0.7473\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4992, Train F1: 0.7593\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4858, Train F1: 0.7697\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4784, Train F1: 0.7775\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4688, Train F1: 0.7834\n",
            "Val F1: 0.0000\n",
            "Experiment 2 - F1: 0.0000, Accuracy: 0.0959\n",
            "\n",
            "Testing with experiment 3\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6268, Train F1: 0.7294\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5830, Train F1: 0.7294\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5603, Train F1: 0.7208\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5419, Train F1: 0.7356\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5276, Train F1: 0.7377\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.5126, Train F1: 0.7480\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.5004, Train F1: 0.7556\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4878, Train F1: 0.7655\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4709, Train F1: 0.7763\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4552, Train F1: 0.7863\n",
            "Val F1: 0.0000\n",
            "Experiment 3 - F1: 0.0000, Accuracy: 0.5963\n",
            "\n",
            "Testing with experiment 4\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6260, Train F1: 0.6435\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5769, Train F1: 0.5969\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5528, Train F1: 0.6163\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5396, Train F1: 0.6564\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5265, Train F1: 0.6982\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.5170, Train F1: 0.7057\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4985, Train F1: 0.7287\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4825, Train F1: 0.7545\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4630, Train F1: 0.7680\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4459, Train F1: 0.7827\n",
            "Val F1: 0.0000\n",
            "Experiment 4 - F1: 0.0000, Accuracy: 0.6711\n",
            "\n",
            "Testing with experiment 5\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6306, Train F1: 0.6362\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5842, Train F1: 0.5730\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5553, Train F1: 0.6172\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5279, Train F1: 0.7023\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.5057, Train F1: 0.7318\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.4863, Train F1: 0.7453\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4720, Train F1: 0.7603\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4552, Train F1: 0.7743\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4455, Train F1: 0.7798\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.4309, Train F1: 0.7901\n",
            "Val F1: 0.0000\n",
            "Experiment 5 - F1: 0.0000, Accuracy: 0.5693\n",
            "\n",
            "Testing with experiment 11\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6157, Train F1: 0.7632\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5786, Train F1: 0.7699\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5491, Train F1: 0.7663\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5205, Train F1: 0.7782\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.4862, Train F1: 0.8003\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.4518, Train F1: 0.8122\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4325, Train F1: 0.8230\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4166, Train F1: 0.8244\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.3999, Train F1: 0.8346\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.3877, Train F1: 0.8390\n",
            "Val F1: 0.0000\n",
            "Experiment 11 - F1: 0.0000, Accuracy: 0.1426\n",
            "\n",
            "Testing with experiment 12\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6184, Train F1: 0.7594\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5766, Train F1: 0.7698\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5435, Train F1: 0.7627\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5131, Train F1: 0.7687\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.4777, Train F1: 0.7897\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.4411, Train F1: 0.8116\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.3964, Train F1: 0.8335\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.3678, Train F1: 0.8444\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.3476, Train F1: 0.8553\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.3259, Train F1: 0.8652\n",
            "Val F1: 0.0000\n",
            "Experiment 12 - F1: 0.0000, Accuracy: 0.0817\n",
            "\n",
            "Testing with experiment 17\n",
            "\n",
            "Epoch 1:\n",
            "Train Loss: 0.6228, Train F1: 0.7547\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 2:\n",
            "Train Loss: 0.5835, Train F1: 0.7549\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 3:\n",
            "Train Loss: 0.5455, Train F1: 0.7614\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 4:\n",
            "Train Loss: 0.5082, Train F1: 0.7895\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 5:\n",
            "Train Loss: 0.4802, Train F1: 0.8003\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 6:\n",
            "Train Loss: 0.4551, Train F1: 0.8132\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 7:\n",
            "Train Loss: 0.4355, Train F1: 0.8176\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 8:\n",
            "Train Loss: 0.4190, Train F1: 0.8251\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 9:\n",
            "Train Loss: 0.4075, Train F1: 0.8299\n",
            "Val F1: 0.0000\n",
            "\n",
            "Epoch 10:\n",
            "Train Loss: 0.3891, Train F1: 0.8388\n",
            "Val F1: 0.0000\n",
            "Experiment 17 - F1: 0.0000, Accuracy: 0.2707\n",
            "\n",
            "Overall LOOCV Results:\n",
            "Average F1-Score: 0.3236\n",
            "Average Accuracy: 0.4376\n",
            "\n",
            "Individual Experiment Scores:\n",
            "Experiment 6: F1 = 0.3147, Accuracy = 0.1867\n",
            "Experiment 7: F1 = 0.7771, Accuracy = 0.6354\n",
            "Experiment 8: F1 = 0.7888, Accuracy = 0.6512\n",
            "Experiment 9: F1 = 0.8179, Accuracy = 0.6919\n",
            "Experiment 10: F1 = 0.9808, Accuracy = 0.9623\n",
            "Experiment 13: F1 = 0.0939, Accuracy = 0.0493\n",
            "Experiment 14: F1 = 0.4698, Accuracy = 0.3070\n",
            "Experiment 15: F1 = 0.7510, Accuracy = 0.6013\n",
            "Experiment 16: F1 = 0.5065, Accuracy = 0.3391\n",
            "Experiment 18: F1 = 0.0000, Accuracy = 0.5867\n",
            "Experiment 1: F1 = 0.0000, Accuracy = 0.0959\n",
            "Experiment 2: F1 = 0.0000, Accuracy = 0.5963\n",
            "Experiment 3: F1 = 0.0000, Accuracy = 0.6711\n",
            "Experiment 4: F1 = 0.0000, Accuracy = 0.5693\n",
            "Experiment 5: F1 = 0.0000, Accuracy = 0.1426\n",
            "Experiment 11: F1 = 0.0000, Accuracy = 0.0817\n",
            "Experiment 12: F1 = 0.0000, Accuracy = 0.2707\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}